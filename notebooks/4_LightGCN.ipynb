{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Graph Convolution Network (LightGCN)\n",
    "\n",
    "This is a TensorFlow implementation of LightGCN with a custom training loop.\n",
    "\n",
    "The LightGCN is adapted from Neural Graph Collaborative Filtering (NGCF). It operates by:\n",
    "\n",
    "1. Graph-Based Approach: Treating data as a graph with nodes (users, items) and edges (interactions like ratings), it captures complex relationships in the data.\n",
    "2. Simplified Convolution Layers: LightGCN uses graph convolution layers to blend features from neighboring nodes, streamlining the process by omitting feature transformations and nonlinear activations.\n",
    "\n",
    "LightGCN's simplicity allows it to effectively harness the graph structure, enhancing its ability to make accurate recommendations. In applications like Goodreads book recommendations, it adeptly uses user-book connections to suggest titles that closely match a user's preferences and reading habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the parent directory to sys.path for relative imports\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from src.utils import preprocess, metrics\n",
    "from src.models import LightGCN\n",
    "\n",
    "# Suppress warnings for cleaner notebook presentation\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "This LightGCN implementation takes an adjacency matrix in a sparse tensor format as input.\n",
    "\n",
    "In preparation of the data for LightGCN, we must:\n",
    "\n",
    "Download the data\n",
    "Stratified train test split\n",
    "Create a normalized adjacency matrix\n",
    "Convert to tensor\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "The data we use is the benchmark MovieLens 100K Dataset, with 100k ratings, 1000 users, and 1700 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (91226, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74505</th>\n",
       "      <td>2540</td>\n",
       "      <td>A Game of Thrones (A Song of Ice and Fire, #1)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60643</th>\n",
       "      <td>5886</td>\n",
       "      <td>The Amazing Adventures of Kavalier &amp; Clay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87603</th>\n",
       "      <td>4411</td>\n",
       "      <td>The World to Come</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81524</th>\n",
       "      <td>4934</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone (Harr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60556</th>\n",
       "      <td>5791</td>\n",
       "      <td>Bloodsucking Fiends (A Love Story, #1)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                          book_name  rating\n",
       "74505     2540     A Game of Thrones (A Song of Ice and Fire, #1)       4\n",
       "60643     5886          The Amazing Adventures of Kavalier & Clay       4\n",
       "87603     4411                                  The World to Come       5\n",
       "81524     4934  Harry Potter and the Philosopher's Stone (Harr...       5\n",
       "60556     5791             Bloodsucking Fiends (A Love Story, #1)       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading ratings data\n",
    "rating_file = os.path.join('..', 'src', 'data', 'goodreads_2m', 'ratings.csv')\n",
    "ratings = pd.read_csv(rating_file)\n",
    "\n",
    "# Displaying the shape of the dataset and a random sample of 5 entries\n",
    "print(f'Shape: {ratings.shape}')\n",
    "ratings.sample(5, random_state=123)  # Setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "\n",
    "We split the data using a stratified split so the users in the training set are also the same users in the test set. LightGCN is not able to generate recommendations for users not yet seen in the training set.\n",
    "\n",
    "Here we will have a training size of 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (68435, 3)\n",
      "Test Shape: (22791, 3)\n",
      "Do they have the same users?: True\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "train_size = 0.75\n",
    "train, test = preprocess.stratified_split(ratings, 'user_id', train_size)\n",
    "print(f'Train Shape: {train.shape}\\nTest Shape: {test.shape}')\n",
    "print(f'Do they have the same users?: {set(train.user_id) == set(test.user_id)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindex\n",
    "\n",
    "Reset the index of users and movies from 0-n for both the training and test data. This is to allow better tracking of users and movies. Dictionaries are created so we can easily translate back and forth from the old index to the new index.\n",
    "\n",
    "We would also normally remove users with no ratings, but in this case, all entries have a user and a rating between 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 1371\n",
      "Number of books: 2720\n",
      "NaNs in train_reindex user_id_new: 0\n",
      "NaNs in test_reindex user_id_new: 0\n"
     ]
    }
   ],
   "source": [
    "# Assuming train and test DataFrames are already defined\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine train and test data\n",
    "combined = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# Count unique users and movies\n",
    "n_users = combined['user_id'].nunique()\n",
    "n_movies = combined['book_name'].nunique()\n",
    "print('Number of users:', n_users)\n",
    "print('Number of books:', n_movies)\n",
    "\n",
    "# Create user and item mappings\n",
    "user2id = {uid: idx for idx, uid in enumerate(combined['user_id'].unique())}\n",
    "book2id = {book: idx for idx, book in enumerate(combined['book_name'].unique())}\n",
    "id2user = {idx: uid for uid, idx in user2id.items()}\n",
    "id2item = {idx: book for book, idx in book2id.items()}\n",
    "\n",
    "# Apply mappings to train and test sets\n",
    "train['user_id_new'] = train['user_id'].map(user2id)\n",
    "train['book_name_new'] = train['book_name'].map(book2id)\n",
    "test['user_id_new'] = test['user_id'].map(user2id)\n",
    "test['book_name_new'] = test['book_name'].map(book2id)\n",
    "\n",
    "# Check for NaNs after reindexing\n",
    "print(\"NaNs in train_reindex user_id_new:\", train['user_id_new'].isna().sum())\n",
    "print(\"NaNs in test_reindex user_id_new:\", test['user_id_new'].isna().sum())\n",
    "\n",
    "# Create a DataFrame to keep track of which books each user has interacted with\n",
    "interacted = train.groupby(\"user_id_new\")[\"book_name_new\"].apply(set).reset_index()\n",
    "interacted.rename(columns={\"book_name_new\": \"book_interacted\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency matrix\n",
    "\n",
    "The adjacency matrix is a data structure the represents a graph by encoding the connections and between nodes. In our case, nodes are both users and movies. Rows and columns consist of ALL the nodes and for every connection (reviewed movie) there is the value 1.\n",
    "\n",
    "To first create the adjacency matrix we first create a user-item graph where similar to the adjacency matrix, connected users and movies are represented as 1 in a sparse array. Unlike the adjacency matrix, a user-item graph only has users for the columns/rows and items as the other, whereas the adjacency matrix has both users and items concatenated as rows and columns.\n",
    "\n",
    "In this case, because the graph is undirected (meaning the connections between nodes do not have a specified direction) the adjacency matrix is symmetric. We use this to our advantage by transposing the user-item graph to create the adjacency matrix.\n",
    "\n",
    "Our adjacency matrix will not include self-connections where each node is connected to itself.\n",
    "\n",
    "# Create adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-item interaction matrix\n",
    "R = sp.dok_matrix((n_users, n_movies), dtype=np.float32)\n",
    "for _, row in train.iterrows():\n",
    "    R[row['user_id_new'], row['book_name_new']] = 1\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_mat = sp.dok_matrix((n_users + n_movies, n_users + n_movies), dtype=np.float32)\n",
    "adj_mat[:n_users, n_users:] = R\n",
    "adj_mat[n_users:, :n_users] = R.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize adjacency matrix\n",
    "\n",
    "This helps numerically stabilize values when repeating graph convolution operations, avoiding the scale of the embeddings increasing or decreasing.\n",
    " \n",
    " \n",
    "\n",
    " is the degree/diagonal matrix where it is zero everywhere but it's diagonal. The diagonal has the value of the neighborhood size of each node (how many other nodes that node connects to)\n",
    "\n",
    " \n",
    " on the left side scales  by the source node, while \n",
    " \n",
    " right side scales by the neighborhood size of the destination node rather than the source node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized adjacency matrix\n",
    "D_values = np.array(adj_mat.sum(1))\n",
    "D_inv_values = np.power(D_values + 1e-9, -0.5).flatten()\n",
    "D_inv_values[np.isinf(D_inv_values)] = 0.0\n",
    "D_inv_sq_root = sp.diags(D_inv_values)\n",
    "norm_adj_mat = D_inv_sq_root.dot(adj_mat).dot(D_inv_sq_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to SparseTensor for TensorFlow\n",
    "coo = norm_adj_mat.tocoo().astype(np.float32)\n",
    "indices = np.mat([coo.row, coo.col]).transpose()\n",
    "A_tilde = tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGCN\n",
    "\n",
    "LightGCN keeps neighbor aggregation while removing self-connections, feature transformation, and nonlinear activation, simplifying as well as improving performance.\n",
    "\n",
    "Neighbor aggregation is done through graph convolutions to learn embeddings that represent nodes. The size of the embeddings can be changed to whatever number. In this notebook, we set the embedding dimension to 64.\n",
    "\n",
    "In matrix form, graph convolution can be thought of as matrix multiplication. In the implementation we create a graph convolution layer that performs just this, allowing us to stack as many graph convolutions as we want. We have the number of layers as 10 in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training\n",
    "\n",
    "For training, we batch a number of users from the training set and sample a single positive item (movie that has been reviewed) and a single negative item (movie that has not been reviewed) for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "N_LAYERS = 10\n",
    "EMBED_DIM = 64\n",
    "DECAY = 0.0001\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# Initialize LightGCN model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model = LightGCN.LightGCN(A_tilde, n_users, n_movies, N_LAYERS, EMBED_DIM, DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "67/67 [==============================] - 24s 362ms/step - training loss: 0.5262\n",
      "Epoch 2/200\n",
      "67/67 [==============================] - 24s 360ms/step - training loss: 0.4538\n",
      "Epoch 3/200\n",
      "67/67 [==============================] - 24s 360ms/step - training loss: 0.4341\n",
      "Epoch 4/200\n",
      "67/67 [==============================] - 24s 361ms/step - training loss: 0.3937\n",
      "Epoch 5/200\n",
      "67/67 [==============================] - 24s 362ms/step - training loss: 0.3652\n",
      "Epoch 6/200\n",
      "67/67 [==============================] - 24s 362ms/step - training loss: 0.3520\n",
      "Epoch 7/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.3360\n",
      "Epoch 8/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.3226\n",
      "Epoch 9/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.3104\n",
      "Epoch 10/200\n",
      "67/67 [==============================] - 24s 364ms/step - training loss: 0.2988\n",
      "Epoch 11/200\n",
      "67/67 [==============================] - 25s 367ms/step - training loss: 0.2878\n",
      "Epoch 12/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.2839\n",
      "Epoch 13/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.2779\n",
      "Epoch 14/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.2662\n",
      "Epoch 15/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.2546\n",
      "Epoch 16/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.2518\n",
      "Epoch 17/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.2415\n",
      "Epoch 18/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.2369\n",
      "Epoch 19/200\n",
      "67/67 [==============================] - 25s 379ms/step - training loss: 0.2327\n",
      "Epoch 20/200\n",
      "67/67 [==============================] - 26s 384ms/step - training loss: 0.2283\n",
      "Epoch 21/200\n",
      "67/67 [==============================] - 26s 394ms/step - training loss: 0.2296\n",
      "Epoch 22/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.2240\n",
      "Epoch 23/200\n",
      "67/67 [==============================] - 26s 385ms/step - training loss: 0.2203\n",
      "Epoch 24/200\n",
      "67/67 [==============================] - 26s 389ms/step - training loss: 0.2202\n",
      "Epoch 25/200\n",
      "67/67 [==============================] - 26s 383ms/step - training loss: 0.2200\n",
      "Epoch 26/200\n",
      "67/67 [==============================] - 26s 382ms/step - training loss: 0.2169\n",
      "Epoch 27/200\n",
      "67/67 [==============================] - 26s 390ms/step - training loss: 0.2109\n",
      "Epoch 28/200\n",
      "67/67 [==============================] - 26s 390ms/step - training loss: 0.2106\n",
      "Epoch 29/200\n",
      "67/67 [==============================] - 26s 394ms/step - training loss: 0.2088\n",
      "Epoch 30/200\n",
      "67/67 [==============================] - 30s 444ms/step - training loss: 0.2068\n",
      "Epoch 31/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.2056\n",
      "Epoch 32/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.2023\n",
      "Epoch 33/200\n",
      "67/67 [==============================] - 25s 381ms/step - training loss: 0.2002\n",
      "Epoch 34/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1971\n",
      "Epoch 35/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.1994\n",
      "Epoch 36/200\n",
      "67/67 [==============================] - 24s 361ms/step - training loss: 0.1949\n",
      "Epoch 37/200\n",
      "67/67 [==============================] - 24s 358ms/step - training loss: 0.1924\n",
      "Epoch 38/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1902\n",
      "Epoch 39/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1860\n",
      "Epoch 40/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1897\n",
      "Epoch 41/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1861\n",
      "Epoch 42/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1835\n",
      "Epoch 43/200\n",
      "67/67 [==============================] - 24s 358ms/step - training loss: 0.1817\n",
      "Epoch 44/200\n",
      "67/67 [==============================] - 24s 359ms/step - training loss: 0.1817\n",
      "Epoch 45/200\n",
      "67/67 [==============================] - 24s 362ms/step - training loss: 0.1783\n",
      "Epoch 46/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1738\n",
      "Epoch 47/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1762\n",
      "Epoch 48/200\n",
      "67/67 [==============================] - 25s 367ms/step - training loss: 0.1698\n",
      "Epoch 49/200\n",
      "67/67 [==============================] - 25s 366ms/step - training loss: 0.1707\n",
      "Epoch 50/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1693\n",
      "Epoch 51/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1675\n",
      "Epoch 52/200\n",
      "67/67 [==============================] - 24s 366ms/step - training loss: 0.1644\n",
      "Epoch 53/200\n",
      "67/67 [==============================] - 25s 366ms/step - training loss: 0.1627\n",
      "Epoch 54/200\n",
      "67/67 [==============================] - 25s 377ms/step - training loss: 0.1611\n",
      "Epoch 55/200\n",
      "67/67 [==============================] - 24s 360ms/step - training loss: 0.1617\n",
      "Epoch 56/200\n",
      "67/67 [==============================] - 24s 361ms/step - training loss: 0.1582\n",
      "Epoch 57/200\n",
      "67/67 [==============================] - 24s 361ms/step - training loss: 0.1546\n",
      "Epoch 58/200\n",
      "67/67 [==============================] - 24s 360ms/step - training loss: 0.1526\n",
      "Epoch 59/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1533\n",
      "Epoch 60/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1527\n",
      "Epoch 61/200\n",
      "67/67 [==============================] - 27s 399ms/step - training loss: 0.1494\n",
      "Epoch 62/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1487\n",
      "Epoch 63/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1474\n",
      "Epoch 64/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1473\n",
      "Epoch 65/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.1458\n",
      "Epoch 66/200\n",
      "67/67 [==============================] - 25s 379ms/step - training loss: 0.1442\n",
      "Epoch 67/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1417\n",
      "Epoch 68/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1433\n",
      "Epoch 69/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1394\n",
      "Epoch 70/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1394\n",
      "Epoch 71/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1378\n",
      "Epoch 72/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1371\n",
      "Epoch 73/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1370\n",
      "Epoch 74/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1353\n",
      "Epoch 75/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1357\n",
      "Epoch 76/200\n",
      "67/67 [==============================] - 24s 354ms/step - training loss: 0.1337\n",
      "Epoch 77/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1345\n",
      "Epoch 78/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1333\n",
      "Epoch 79/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1311\n",
      "Epoch 80/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1307\n",
      "Epoch 81/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.1294\n",
      "Epoch 82/200\n",
      "67/67 [==============================] - 24s 358ms/step - training loss: 0.1294\n",
      "Epoch 83/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1283\n",
      "Epoch 84/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1281\n",
      "Epoch 85/200\n",
      "67/67 [==============================] - 24s 362ms/step - training loss: 0.1261\n",
      "Epoch 86/200\n",
      "67/67 [==============================] - 24s 358ms/step - training loss: 0.1265\n",
      "Epoch 87/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1274\n",
      "Epoch 88/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1251\n",
      "Epoch 89/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1240\n",
      "Epoch 90/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1237\n",
      "Epoch 91/200\n",
      "67/67 [==============================] - 26s 390ms/step - training loss: 0.1228\n",
      "Epoch 92/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1230\n",
      "Epoch 93/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1217\n",
      "Epoch 94/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1207\n",
      "Epoch 95/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1219\n",
      "Epoch 96/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1224\n",
      "Epoch 97/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1212\n",
      "Epoch 98/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1187\n",
      "Epoch 99/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1188\n",
      "Epoch 100/200\n",
      "67/67 [==============================] - 25s 380ms/step - training loss: 0.1178\n",
      "Epoch 101/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1176\n",
      "Epoch 102/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1173\n",
      "Epoch 103/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1178\n",
      "Epoch 104/200\n",
      "67/67 [==============================] - 25s 377ms/step - training loss: 0.1169\n",
      "Epoch 105/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1157\n",
      "Epoch 106/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1163\n",
      "Epoch 107/200\n",
      "67/67 [==============================] - 25s 374ms/step - training loss: 0.1146\n",
      "Epoch 108/200\n",
      "67/67 [==============================] - 25s 370ms/step - training loss: 0.1137\n",
      "Epoch 109/200\n",
      "67/67 [==============================] - 26s 383ms/step - training loss: 0.1143\n",
      "Epoch 110/200\n",
      "67/67 [==============================] - 26s 392ms/step - training loss: 0.1133\n",
      "Epoch 111/200\n",
      "67/67 [==============================] - 28s 422ms/step - training loss: 0.1122\n",
      "Epoch 112/200\n",
      "67/67 [==============================] - 25s 380ms/step - training loss: 0.1145\n",
      "Epoch 113/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.1131\n",
      "Epoch 114/200\n",
      "67/67 [==============================] - 26s 382ms/step - training loss: 0.1138\n",
      "Epoch 115/200\n",
      "67/67 [==============================] - 27s 397ms/step - training loss: 0.1124\n",
      "Epoch 116/200\n",
      "67/67 [==============================] - 26s 386ms/step - training loss: 0.1102\n",
      "Epoch 117/200\n",
      "67/67 [==============================] - 26s 388ms/step - training loss: 0.1114\n",
      "Epoch 118/200\n",
      "67/67 [==============================] - 27s 402ms/step - training loss: 0.1128\n",
      "Epoch 119/200\n",
      "67/67 [==============================] - 26s 386ms/step - training loss: 0.1113\n",
      "Epoch 120/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.1098\n",
      "Epoch 121/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1100\n",
      "Epoch 122/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.1097\n",
      "Epoch 123/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1100\n",
      "Epoch 124/200\n",
      "67/67 [==============================] - 25s 379ms/step - training loss: 0.1106\n",
      "Epoch 125/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1094\n",
      "Epoch 126/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1085\n",
      "Epoch 127/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1083\n",
      "Epoch 128/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1095\n",
      "Epoch 129/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1089\n",
      "Epoch 130/200\n",
      "67/67 [==============================] - 25s 378ms/step - training loss: 0.1091\n",
      "Epoch 131/200\n",
      "67/67 [==============================] - 25s 377ms/step - training loss: 0.1083\n",
      "Epoch 132/200\n",
      "67/67 [==============================] - 25s 378ms/step - training loss: 0.1077\n",
      "Epoch 133/200\n",
      "67/67 [==============================] - 26s 394ms/step - training loss: 0.1084\n",
      "Epoch 134/200\n",
      "67/67 [==============================] - 26s 387ms/step - training loss: 0.1091\n",
      "Epoch 135/200\n",
      "67/67 [==============================] - 25s 379ms/step - training loss: 0.1082\n",
      "Epoch 136/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.1078\n",
      "Epoch 137/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1075\n",
      "Epoch 138/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1059\n",
      "Epoch 139/200\n",
      "67/67 [==============================] - 26s 392ms/step - training loss: 0.1068\n",
      "Epoch 140/200\n",
      "67/67 [==============================] - 26s 379ms/step - training loss: 0.1065\n",
      "Epoch 141/200\n",
      "67/67 [==============================] - 26s 389ms/step - training loss: 0.1081\n",
      "Epoch 142/200\n",
      "67/67 [==============================] - 26s 391ms/step - training loss: 0.1064\n",
      "Epoch 143/200\n",
      "67/67 [==============================] - 25s 376ms/step - training loss: 0.1062\n",
      "Epoch 144/200\n",
      "67/67 [==============================] - 26s 394ms/step - training loss: 0.1057\n",
      "Epoch 145/200\n",
      "67/67 [==============================] - 28s 416ms/step - training loss: 0.1060\n",
      "Epoch 146/200\n",
      "67/67 [==============================] - 26s 387ms/step - training loss: 0.1050\n",
      "Epoch 147/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1038\n",
      "Epoch 148/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1047\n",
      "Epoch 149/200\n",
      "67/67 [==============================] - 26s 381ms/step - training loss: 0.1055\n",
      "Epoch 150/200\n",
      "67/67 [==============================] - 25s 377ms/step - training loss: 0.1043\n",
      "Epoch 151/200\n",
      "67/67 [==============================] - 25s 379ms/step - training loss: 0.1040\n",
      "Epoch 152/200\n",
      "67/67 [==============================] - 25s 378ms/step - training loss: 0.1041\n",
      "Epoch 153/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1032\n",
      "Epoch 154/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1052\n",
      "Epoch 155/200\n",
      "67/67 [==============================] - 25s 368ms/step - training loss: 0.1040\n",
      "Epoch 156/200\n",
      "67/67 [==============================] - 25s 369ms/step - training loss: 0.1054\n",
      "Epoch 157/200\n",
      "67/67 [==============================] - 26s 381ms/step - training loss: 0.1041\n",
      "Epoch 158/200\n",
      "67/67 [==============================] - 26s 387ms/step - training loss: 0.1046\n",
      "Epoch 159/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1035\n",
      "Epoch 160/200\n",
      "67/67 [==============================] - 24s 360ms/step - training loss: 0.1041\n",
      "Epoch 161/200\n",
      "67/67 [==============================] - 25s 366ms/step - training loss: 0.1024\n",
      "Epoch 162/200\n",
      "67/67 [==============================] - 24s 355ms/step - training loss: 0.1034\n",
      "Epoch 163/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.1038\n",
      "Epoch 164/200\n",
      "67/67 [==============================] - 25s 374ms/step - training loss: 0.1025\n",
      "Epoch 165/200\n",
      "67/67 [==============================] - 28s 412ms/step - training loss: 0.1028\n",
      "Epoch 166/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1024\n",
      "Epoch 167/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.1045\n",
      "Epoch 168/200\n",
      "67/67 [==============================] - 24s 361ms/step - training loss: 0.1018\n",
      "Epoch 169/200\n",
      "67/67 [==============================] - 24s 364ms/step - training loss: 0.1026\n",
      "Epoch 170/200\n",
      "67/67 [==============================] - 24s 364ms/step - training loss: 0.1024\n",
      "Epoch 171/200\n",
      "67/67 [==============================] - 24s 363ms/step - training loss: 0.1033\n",
      "Epoch 172/200\n",
      "67/67 [==============================] - 25s 367ms/step - training loss: 0.1019\n",
      "Epoch 173/200\n",
      "67/67 [==============================] - 25s 377ms/step - training loss: 0.1023\n",
      "Epoch 174/200\n",
      "67/67 [==============================] - 25s 380ms/step - training loss: 0.1030\n",
      "Epoch 175/200\n",
      "67/67 [==============================] - 25s 378ms/step - training loss: 0.1008\n",
      "Epoch 176/200\n",
      "67/67 [==============================] - 25s 367ms/step - training loss: 0.1017\n",
      "Epoch 177/200\n",
      "67/67 [==============================] - 24s 359ms/step - training loss: 0.1016\n",
      "Epoch 178/200\n",
      "67/67 [==============================] - 25s 381ms/step - training loss: 0.1026\n",
      "Epoch 179/200\n",
      "67/67 [==============================] - 26s 383ms/step - training loss: 0.1020\n",
      "Epoch 180/200\n",
      "67/67 [==============================] - 26s 389ms/step - training loss: 0.1021\n",
      "Epoch 181/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1014\n",
      "Epoch 182/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1017\n",
      "Epoch 183/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.1009\n",
      "Epoch 184/200\n",
      "67/67 [==============================] - 25s 375ms/step - training loss: 0.1015\n",
      "Epoch 185/200\n",
      "67/67 [==============================] - 25s 374ms/step - training loss: 0.1011\n",
      "Epoch 186/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1005\n",
      "Epoch 187/200\n",
      "67/67 [==============================] - 25s 374ms/step - training loss: 0.1009\n",
      "Epoch 188/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1007\n",
      "Epoch 189/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1005\n",
      "Epoch 190/200\n",
      "67/67 [==============================] - 25s 371ms/step - training loss: 0.1008\n",
      "Epoch 191/200\n",
      "67/67 [==============================] - 25s 372ms/step - training loss: 0.1013\n",
      "Epoch 192/200\n",
      "67/67 [==============================] - 25s 373ms/step - training loss: 0.1017\n",
      "Epoch 193/200\n",
      "67/67 [==============================] - 26s 388ms/step - training loss: 0.1005\n",
      "Epoch 194/200\n",
      "67/67 [==============================] - 26s 389ms/step - training loss: 0.0996\n",
      "Epoch 195/200\n",
      "67/67 [==============================] - 24s 364ms/step - training loss: 0.0987\n",
      "Epoch 196/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1006\n",
      "Epoch 197/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1007\n",
      "Epoch 198/200\n",
      "67/67 [==============================] - 24s 358ms/step - training loss: 0.1000\n",
      "Epoch 199/200\n",
      "67/67 [==============================] - 24s 357ms/step - training loss: 0.1005\n",
      "Epoch 200/200\n",
      "67/67 [==============================] - 24s 356ms/step - training loss: 0.1000\n",
      "CPU times: user 1h 55min 39s, sys: 22min 51s, total: 2h 18min 30s\n",
      "Wall time: 1h 23min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Custom training loop with negative sampling and gradient updates\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    n_batch = len(train) // BATCH_SIZE + (len(train) % BATCH_SIZE != 0)\n",
    "    bar = Progbar(n_batch)\n",
    "    \n",
    "    for _ in range(n_batch):\n",
    "        # Sample a batch of users\n",
    "        users = np.random.choice(n_users, BATCH_SIZE, replace=False)\n",
    "\n",
    "        # Function for negative sampling\n",
    "        def sample_neg(user_interacted_items):\n",
    "            while True:\n",
    "                neg_item = random.randint(0, n_movies - 1)\n",
    "                if neg_item not in user_interacted_items:\n",
    "                    return neg_item\n",
    "\n",
    "        # Sample positive and negative items for each user\n",
    "        pos_items = [random.choice(list(interacted[interacted['user_id_new'] == u]['book_interacted'].values[0])) for u in users]\n",
    "        neg_items = [sample_neg(interacted[interacted['user_id_new'] == u]['book_interacted'].values[0]) for u in users]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Call LightGCN with user and item embeddings\n",
    "            new_user_embeddings, new_item_embeddings = model(\n",
    "                (model.user_embedding, model.item_embedding)\n",
    "            )\n",
    "\n",
    "            # Embeddings after convolutions\n",
    "            user_embeddings = tf.nn.embedding_lookup(new_user_embeddings, users)\n",
    "            pos_item_embeddings = tf.nn.embedding_lookup(new_item_embeddings, pos_items)\n",
    "            neg_item_embeddings = tf.nn.embedding_lookup(new_item_embeddings, neg_items)\n",
    "\n",
    "            # Initial embeddings before convolutions\n",
    "            old_user_embeddings = tf.nn.embedding_lookup(model.user_embedding, users)\n",
    "            old_pos_item_embeddings = tf.nn.embedding_lookup(model.item_embedding, pos_items)\n",
    "            old_neg_item_embeddings = tf.nn.embedding_lookup(model.item_embedding, neg_items)\n",
    "\n",
    "            # Calculate loss\n",
    "            pos_scores = tf.reduce_sum(tf.multiply(user_embeddings, pos_item_embeddings), axis=1)\n",
    "            neg_scores = tf.reduce_sum(tf.multiply(user_embeddings, neg_item_embeddings), axis=1)\n",
    "            regularizer = (tf.nn.l2_loss(old_user_embeddings) +\n",
    "                           tf.nn.l2_loss(old_pos_item_embeddings) +\n",
    "                           tf.nn.l2_loss(old_neg_item_embeddings)) / BATCH_SIZE\n",
    "            mf_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
    "            emb_loss = DECAY * regularizer\n",
    "            loss = mf_loss + emb_loss\n",
    "\n",
    "        # Apply gradients\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        bar.add(1, values=[('training loss', float(loss))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>9.685920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code (Robert Langdon, #2)</td>\n",
       "      <td>9.277096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The Tipping Point: How Little Things Can Make ...</td>\n",
       "      <td>8.906483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A Tale of Two Cities</td>\n",
       "      <td>8.489778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Brave New World</td>\n",
       "      <td>8.306227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          book_name  prediction\n",
       "0        1                              To Kill a Mockingbird    9.685920\n",
       "1        1             The Da Vinci Code (Robert Langdon, #2)    9.277096\n",
       "2        1  The Tipping Point: How Little Things Can Make ...    8.906483\n",
       "3        1                               A Tale of Two Cities    8.489778\n",
       "4        1                                    Brave New World    8.306227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate recommendations\n",
    "users = np.array([user2id[x] for x in test['user_id'].unique()])\n",
    "recommendations = model.recommend(users, k=10)\n",
    "\n",
    "# Replace new user and book IDs with original IDs\n",
    "recommendations['user_id'] = recommendations['user_id'].map(id2user)\n",
    "recommendations['book_name'] = recommendations['book_name'].map(id2item)\n",
    "\n",
    "# Display the first 5 recommendations\n",
    "recommendations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The performance of our model is evaluated using the test set, which consists of the exact same users in the training set but with books the users have reviewed that the model has not seen before. A good model will recommend books that the user has also reviewed in the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "Out of the books that are recommended, what proportion is relevant. Relevant in this case is if the user has reviewed the book.\n",
    "\n",
    "A precision@10 of about 0.1 means that about 10% of the recommendations are relevant to the user. In other words, out of the 10 recommendations made, on average a user will have 1 book that is actually relevant.\n",
    "\n",
    "### Recall@k\n",
    "\n",
    "Out of all the relevant books (in the test set), how many are recommended.\n",
    "\n",
    "A recall@10 of 0.1 means that about 10% of the relevant books were recommended. By definition you can see how even if all the recommendations made were relevant, recall@k is capped by k. A higher k means that more relevant books can be recommended.\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "Calculate the average precision for each user and average all the average precisions over all users. Penalizes incorrect rankings of books.\n",
    "\n",
    "### Normalized Discounted Cumulative Gain (NDGC)\n",
    "\n",
    "Looks at both relevant books and the ranking order of the relevant books. Normalized by the total number of users.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.200073\n",
      "Recall: 0.148815\n",
      "MAP: 0.080734\n",
      "NDCG: 0.240993\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "top_k = recommendations.copy()\n",
    "top_k['rank'] = top_k.groupby('user_id', sort=False).cumcount() + 1\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision_at_k = metrics.precision_at_k(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "recall_at_k = metrics.recall_at_k(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "mean_average_precision = metrics.mean_average_precision(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "ndcg = metrics.ndcg(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f'Precision: {precision_at_k:.6f}',\n",
    "      f'Recall: {recall_at_k:.6f}',\n",
    "      f'MAP: {mean_average_precision:.6f}',\n",
    "      f'NDCG: {ndcg:.6f}', sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
