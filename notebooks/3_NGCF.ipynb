{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Graph Collaborative Filtering (NGCF)\n",
    "\n",
    "Neural Graph Collaborative Filtering (NGCF) is a recommendation algorithm that uses a graph-based approach to improve recommendation quality. It constructs a user-item bipartite graph and refines user and item embeddings through a graph neural network. This process captures complex user-item relationships by considering high-order connectivity in the graph, leading to more accurate and relevant recommendations. NGCF is particularly effective in scenarios with rich and intricate user-item interaction data.\n",
    "\n",
    "This is a TensorFlow implementation of NGCF with a custom training loop.\n",
    "\n",
    "Neural Graph Collaborative Filtering (NGCF) is a state-of-the-art GCN-based recommender model that takes advantage of graph structure and is a precursor to the superior LightGCN. In this notebook, we construct and train an NGCF model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the parent directory to sys.path for relative imports\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from src.utils import preprocess, metrics\n",
    "from src.models import NGCF\n",
    "\n",
    "# Suppress warnings for cleaner notebook presentation\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "This NGCF implementation takes an adjacency matrix in a sparse tensor format as input.\n",
    "\n",
    "In preparation of the data for NGCF, we must:\n",
    "\n",
    "* Stratified train test split\n",
    "* Create a normalized adjacency matrix\n",
    "* Convert to tensor\n",
    "* Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (91226, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74505</th>\n",
       "      <td>2540</td>\n",
       "      <td>A Game of Thrones (A Song of Ice and Fire, #1)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60643</th>\n",
       "      <td>5886</td>\n",
       "      <td>The Amazing Adventures of Kavalier &amp; Clay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87603</th>\n",
       "      <td>4411</td>\n",
       "      <td>The World to Come</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81524</th>\n",
       "      <td>4934</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone (Harr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60556</th>\n",
       "      <td>5791</td>\n",
       "      <td>Bloodsucking Fiends (A Love Story, #1)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                          book_name  rating\n",
       "74505     2540     A Game of Thrones (A Song of Ice and Fire, #1)       4\n",
       "60643     5886          The Amazing Adventures of Kavalier & Clay       4\n",
       "87603     4411                                  The World to Come       5\n",
       "81524     4934  Harry Potter and the Philosopher's Stone (Harr...       5\n",
       "60556     5791             Bloodsucking Fiends (A Love Story, #1)       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading ratings data\n",
    "rating_file = os.path.join('..', 'src', 'data', 'goodreads_2m', 'ratings.csv')\n",
    "ratings = pd.read_csv(rating_file)\n",
    "\n",
    "# Displaying the shape of the dataset and a random sample of 5 entries\n",
    "print(f'Shape: {ratings.shape}')\n",
    "ratings.sample(5, random_state=123)  # Setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "\n",
    "We split the data using a stratified split so the users in the training set are also the same users in the test set. NGCF is not able to generate recommendations for users not yet seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (68435, 3)\n",
      "Test Set Shape: (22791, 3)\n",
      "Do they have the same users?: True\n",
      "Number of Users: 1371\n",
      "Number of Books: 2720\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test sets\n",
    "train_size = 0.75\n",
    "train, test = preprocess.stratified_split(ratings, 'user_id', train_size)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Displaying train and test set details\n",
    "print(f'Train Set Shape: {train.shape}')\n",
    "print(f'Test Set Shape: {test.shape}')\n",
    "print(f'Do they have the same users?: {set(train.user_id) == set(test.user_id)}')\n",
    "\n",
    "# Combining train and test data for global statistics\n",
    "combined = pd.concat([train, test]).reset_index(drop=True)\n",
    "n_users = combined['user_id'].nunique()\n",
    "n_books = combined['book_name'].nunique()\n",
    "\n",
    "# Displaying global statistics\n",
    "print(f'Number of Users: {n_users}')\n",
    "print(f'Number of Books: {n_books}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex\n",
    "\n",
    "Reset the index of users and books from 0-n for both the training and test data. This is to allow better tracking of users and books. Dictionaries are created so we can easily translate back and forth from the old index to the new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing users and books\n",
    "unique_user_ids = combined['user_id'].unique()\n",
    "user2id = {user_id: i for i, user_id in enumerate(unique_user_ids)}\n",
    "\n",
    "# Create a new DataFrame for unique books with new indices\n",
    "book_new = combined[['book_name']].drop_duplicates().reset_index(drop=True)\n",
    "book_new['book_name_new'] = np.arange(len(book_new))\n",
    "\n",
    "# Function to reindex DataFrame\n",
    "def reindex_df(df, book_new, user2id):\n",
    "    df_reindex = pd.merge(df, book_new, on='book_name', how='left')\n",
    "    df_reindex['user_id_new'] = df['user_id'].map(user2id)\n",
    "    return df_reindex[['user_id_new', 'book_name_new', 'rating']]\n",
    "\n",
    "# Applying the reindexing to train and test sets\n",
    "train_reindex = reindex_df(train, book_new, user2id)\n",
    "test_reindex = reindex_df(test, book_new, user2id)\n",
    "\n",
    "# Creating mapping dictionaries\n",
    "item2id = dict(zip(book_new['book_name'], book_new['book_name_new']))\n",
    "id2item = dict(zip(book_new['book_name_new'], book_new['book_name']))\n",
    "id2user = dict(zip(train_reindex['user_id_new'], train['user_id']))\n",
    "\n",
    "# Grouping interacted items by users\n",
    "interacted = train_reindex.groupby(\"user_id_new\")[\"book_name_new\"].apply(set).reset_index().rename(columns={\"book_name_new\": \"book_interacted\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency matrix\n",
    "\n",
    "In our case, nodes are both users and books. Rows and columns consist of ALL the nodes and for every connection (reviewed book) there is the value 1.\n",
    "\n",
    "To first create the adjacency matrix we first create a user-item graph where similar to the adjacency matrix, connected users and books are represented as 1 in a sparse array. Unlike the adjacency matrix, a user-item graph only has users for the columns/rows and items as the other, whereas the adjacency matrix has both users and items concatenated as rows and columns.\n",
    "\n",
    "In this case, because the graph is undirected (meaning the connections between nodes do not have a specified direction) the adjacency matrix is symmetric. We use this to our advantage by transposing the user-item graph to create the adjacency matrix.\n",
    "\n",
    "Our adjacency matrix will not include self-connections where each node is connected to itself.\n",
    "\n",
    "## Create adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating user-item graph as a sparse matrix\n",
    "R = sp.dok_matrix((n_users, n_books), dtype=np.float32)\n",
    "for _, row in train_reindex.iterrows():\n",
    "    R[row['user_id_new'], row['book_name_new']] = 1\n",
    "\n",
    "# Creating the adjacency matrix\n",
    "adj_mat = sp.dok_matrix((n_users + n_books, n_users + n_books), dtype=np.float32)\n",
    "adj_mat[:n_users, n_users:] = R\n",
    "adj_mat[n_users:, :n_users] = R.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize adjacency matrix\n",
    "\n",
    "This helps numerically stabilize values when repeating graph convolution operations, avoiding the scale of the embeddings increasing or decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the degree matrix\n",
    "D_values = np.array(adj_mat.sum(1))\n",
    "D_inv_values = np.power(D_values + 1e-9, -0.5).flatten()\n",
    "D_inv_values[np.isinf(D_inv_values)] = 0.0\n",
    "D_inv_sq_root = sp.diags(D_inv_values)\n",
    "\n",
    "# Normalizing the adjacency matrix\n",
    "norm_adj_mat = D_inv_sq_root.dot(adj_mat).dot(D_inv_sq_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the normalized adjacency matrix to COO format for TensorFlow SparseTensor\n",
    "coo = norm_adj_mat.tocoo().astype(np.float32)\n",
    "indices = np.hstack((coo.row[:, np.newaxis], coo.col[:, np.newaxis]))\n",
    "A_tilde = tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGCF\n",
    "\n",
    "### Custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 265920\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "N_LAYERS = 5\n",
    "EMBED_DIM = 64\n",
    "DECAY = 0.0001\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# Expected number of parameters\n",
    "print(f'Parameters: {EMBED_DIM**2 + EMBED_DIM * (n_users + n_books)}')\n",
    "\n",
    "# Initialize the NGCF model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model = NGCF.NGCF(adj_mat=A_tilde, R=R, n_users=n_users, n_items=n_books, n_layers=N_LAYERS, emb_dim=EMBED_DIM, decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "67/67 [==============================] - 7s 109ms/step - training loss: 0.5011\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.3943\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.3137\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 7s 109ms/step - training loss: 0.2799\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 8s 116ms/step - training loss: 0.2563\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.2273\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 8s 115ms/step - training loss: 0.2136\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 7s 112ms/step - training loss: 0.2013\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.1896\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 7s 106ms/step - training loss: 0.1853\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 8s 116ms/step - training loss: 0.1828\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 8s 116ms/step - training loss: 0.1754\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 8s 114ms/step - training loss: 0.1700\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 8s 115ms/step - training loss: 0.1598\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 8s 115ms/step - training loss: 0.1521\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 8s 116ms/step - training loss: 0.1479\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 8s 117ms/step - training loss: 0.1381\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 7s 111ms/step - training loss: 0.1381\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 7s 109ms/step - training loss: 0.1337\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 8s 113ms/step - training loss: 0.1284\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 8s 114ms/step - training loss: 0.1269\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.1270\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.1213\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.1198\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.1162\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.1143\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 8s 119ms/step - training loss: 0.1103\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 7s 112ms/step - training loss: 0.1084\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.1044\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 7s 111ms/step - training loss: 0.1056\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 8s 115ms/step - training loss: 0.1027\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 7s 111ms/step - training loss: 0.1011\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 7s 110ms/step - training loss: 0.0980\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 8s 118ms/step - training loss: 0.0971\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 8s 114ms/step - training loss: 0.0970\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 7s 112ms/step - training loss: 0.0930\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 7s 109ms/step - training loss: 0.0918\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.0890\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 7s 105ms/step - training loss: 0.0888\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 7s 104ms/step - training loss: 0.0866\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 7s 104ms/step - training loss: 0.0871\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 7s 104ms/step - training loss: 0.0874\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 7s 104ms/step - training loss: 0.0806\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 7s 103ms/step - training loss: 0.0831\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.0790\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.0806\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 7s 108ms/step - training loss: 0.0806\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 8s 112ms/step - training loss: 0.0812\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 8s 114ms/step - training loss: 0.0790\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 7s 107ms/step - training loss: 0.0783\n",
      "CPU times: user 9min 8s, sys: 1min 47s, total: 10min 56s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sample_neg(interacted_items, n_books):\n",
    "    \"\"\"Function to sample a negative item not interacted by the user\"\"\"\n",
    "    while True:\n",
    "        neg_item = random.randint(0, n_books - 1)\n",
    "        if neg_item not in interacted_items:\n",
    "            return neg_item\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    n_batch = train_reindex.shape[0] // BATCH_SIZE + (train_reindex.shape[0] % BATCH_SIZE != 0)\n",
    "    bar = Progbar(n_batch)\n",
    "\n",
    "    for _ in range(n_batch):\n",
    "        # Sample a batch of users\n",
    "        users = np.random.choice(n_users, BATCH_SIZE, replace=False)\n",
    "\n",
    "        # Sample positive and negative items for each user\n",
    "        pos_items = [random.choice(list(interacted.loc[user]['book_interacted'])) for user in users]\n",
    "        neg_items = [sample_neg(interacted.loc[user]['book_interacted'], n_books) for user in users]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get new embeddings\n",
    "            new_user_embeddings, new_item_embeddings = model(model.user_embedding, model.item_embedding)\n",
    "            # Look up embeddings for sampled users and items\n",
    "            user_embeddings = tf.nn.embedding_lookup(new_user_embeddings, users)\n",
    "            pos_item_embeddings = tf.nn.embedding_lookup(new_item_embeddings, pos_items)\n",
    "            neg_item_embeddings = tf.nn.embedding_lookup(new_item_embeddings, neg_items)\n",
    "\n",
    "            # Look up old embeddings for regularisation term\n",
    "            old_user_embeddings = tf.nn.embedding_lookup(model.user_embedding, users)\n",
    "            old_pos_item_embeddings = tf.nn.embedding_lookup(model.item_embedding, pos_items)\n",
    "            old_neg_item_embeddings = tf.nn.embedding_lookup(model.item_embedding, neg_items)\n",
    "\n",
    "            # Compute scores and loss\n",
    "            pos_scores = tf.reduce_sum(user_embeddings * pos_item_embeddings, axis=1)\n",
    "            neg_scores = tf.reduce_sum(user_embeddings * neg_item_embeddings, axis=1)\n",
    "            mf_loss = tf.reduce_mean(tf.nn.softplus(neg_scores - pos_scores))\n",
    "            emb_loss = DECAY * (tf.nn.l2_loss(old_user_embeddings) + tf.nn.l2_loss(old_pos_item_embeddings) + tf.nn.l2_loss(old_neg_item_embeddings)) / BATCH_SIZE\n",
    "            loss = mf_loss + emb_loss\n",
    "\n",
    "            # Compute gradients and apply them\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            bar.add(1, values=[('training loss', float(loss))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Into the Wild</td>\n",
       "      <td>10.798458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Predictably Irrational: The Hidden Forces That...</td>\n",
       "      <td>10.849976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The Tipping Point: How Little Things Can Make ...</td>\n",
       "      <td>14.096086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code (Robert Langdon, #2)</td>\n",
       "      <td>14.454141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Brave New World</td>\n",
       "      <td>12.045841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          book_name  prediction\n",
       "0        1                                      Into the Wild   10.798458\n",
       "1        1  Predictably Irrational: The Hidden Forces That...   10.849976\n",
       "2        1  The Tipping Point: How Little Things Can Make ...   14.096086\n",
       "3        1             The Da Vinci Code (Robert Langdon, #2)   14.454141\n",
       "4        1                                    Brave New World   12.045841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make recommendations\n",
    "# Convert test user ids to the new ids\n",
    "users = np.array([user2id[x] for x in test['user_id'].unique()])\n",
    "recommendations = model.recommend(users, k=10)\n",
    "recommendations = recommendations.replace({'user_id': id2user, 'book_name': id2item})\n",
    "recommendations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The performance of our model is evaluated using the test set, which consists of the exact same users in the training set but with books the users have reviewed that the model has not seen before. A good model will recommend books that the user has also reviewed in the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "Out of the books that are recommended, what proportion is relevant. Relevant in this case is if the user has reviewed the book.\n",
    "\n",
    "A precision@10 of about 0.1 means that about 10% of the recommendations are relevant to the user. In other words, out of the 10 recommendations made, on average a user will have 1 book that is actually relevant.\n",
    "\n",
    "### Recall@k\n",
    "\n",
    "Out of all the relevant books (in the test set), how many are recommended.\n",
    "\n",
    "A recall@10 of 0.1 means that about 10% of the relevant books were recommended. By definition you can see how even if all the recommendations made were relevant, recall@k is capped by k. A higher k means that more relevant books can be recommended.\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "Calculate the average precision for each user and average all the average precisions over all users. Penalizes incorrect rankings of books.\n",
    "\n",
    "### Normalized Discounted Cumulative Gain (NDGC)\n",
    "\n",
    "Looks at both relevant books and the ranking order of the relevant books. Normalized by the total number of users.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.153611\n",
      "Recall: 0.111269\n",
      "MAP: 0.038892\n",
      "NDCG: 0.151472\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "top_k = recommendations.copy()\n",
    "top_k['rank'] = top_k.groupby('user_id', sort=False).cumcount() + 1\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision_at_k = metrics.precision_at_k(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "recall_at_k = metrics.recall_at_k(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "mean_average_precision = metrics.mean_average_precision(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "ndcg = metrics.ndcg(top_k, test, 'user_id', 'book_name', 'rank')\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f'Precision: {precision_at_k:.6f}',\n",
    "      f'Recall: {recall_at_k:.6f}',\n",
    "      f'MAP: {mean_average_precision:.6f}',\n",
    "      f'NDCG: {ndcg:.6f}', sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
